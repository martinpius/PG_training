{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz70yQNS1Pa8xKGUH6rVtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/PG_training/blob/main/spark_intro_ST7203_Big_Data_Management.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "Wu9zGG9kMzup"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a SparkSession\n",
        "# SparkSession is the entry point to Spark functionality. It is used to initialize Spark.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark Activities Example\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Z2uxlnHbCkdK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a toy dataset\n",
        "# Let's create a list of tuples representing (Name, Age, Gender, Salary)\n",
        "data = [\n",
        "    (\"Alice\", 30, \"Female\", 70000),\n",
        "    (\"Bob\", 35, \"Male\", 80000),\n",
        "    (\"Charlie\", 25, \"Male\", 50000),\n",
        "    (\"Diana\", 29, \"Female\", 60000),\n",
        "    (\"Edward\", 40, \"Male\", 100000)\n",
        "]\n"
      ],
      "metadata": {
        "id": "gQ7CyKhGCpmC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Create an RDD\n",
        "# An RDD (Resilient Distributed Dataset) is the core abstraction of Spark for distributed data processing.\n",
        "rdd = spark.sparkContext.parallelize(data)\n"
      ],
      "metadata": {
        "id": "oZArRJluCv1i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Apply transformations and actions to the RDD\n",
        "# Transformations return a new RDD, and actions compute a result.\n",
        "\n",
        "# Example 1: Count the number of elements in the RDD\n",
        "print(\"Number of records in the RDD:\", rdd.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA3wA_aBC2Vz",
        "outputId": "08f77cc2-86f0-47cd-ab3e-7efc8c7255c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records in the RDD: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example 2: Filter records where age > 30\n",
        "age_filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
        "print(\"Filtered RDD where Age > 30:\")\n",
        "print(age_filtered_rdd.collect())  # collect() returns all elements of the RDD to the driver\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdIj8i6XC71e",
        "outputId": "0c3f1feb-7cbd-4cde-df28-64aee5e22597"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered RDD where Age > 30:\n",
            "[('Bob', 35, 'Male', 80000), ('Edward', 40, 'Male', 100000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Map Transformation - Extract just the names\n",
        "names_rdd = rdd.map(lambda x: x[0])\n",
        "print(\"Names in the dataset:\")\n",
        "print(names_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toMbYAzODBJ8",
        "outputId": "137fdf93-f900-4ee4-bf0b-e8ce052c426e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Names in the dataset:\n",
            "['Alice', 'Bob', 'Charlie', 'Diana', 'Edward']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Reduce - Calculate the total salary\n",
        "total_salary = rdd.map(lambda x: x[3]).reduce(lambda a, b: a + b)\n",
        "print(\"Total salary of all employees:\", total_salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50oB2lYUDF5O",
        "outputId": "8c4109ff-c92a-43e4-fa6e-8745bfdafdbc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total salary of all employees: 360000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Part 2: Convert the RDD to a DataFrame and use Spark SQL\n",
        "# -------------------------------------------\n",
        "\n",
        "# Step 1: Convert RDD to a DataFrame\n",
        "# We need to define column names to structure the DataFrame\n",
        "columns = [\"Name\", \"Age\", \"Gender\", \"Salary\"]\n",
        "df = spark.createDataFrame(rdd, schema=columns)\n"
      ],
      "metadata": {
        "id": "BH0srsCuDJXV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Show the DataFrame\n",
        "# Show the first 5 rows of the DataFrame\n",
        "print(\"DataFrame:\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI7MMx34DN9A",
        "outputId": "1abeeb30-cc2b-437b-acd4-3d76558df0bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame:\n",
            "+-------+---+------+------+\n",
            "|   Name|Age|Gender|Salary|\n",
            "+-------+---+------+------+\n",
            "|  Alice| 30|Female| 70000|\n",
            "|    Bob| 35|  Male| 80000|\n",
            "|Charlie| 25|  Male| 50000|\n",
            "|  Diana| 29|Female| 60000|\n",
            "| Edward| 40|  Male|100000|\n",
            "+-------+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Create a temporary SQL view\n",
        "# This allows us to run SQL queries directly on the DataFrame\n",
        "df.createOrReplaceTempView(\"employees\")\n"
      ],
      "metadata": {
        "id": "wrTCr96_DTUA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Use SQL queries to analyze the data\n",
        "print(\"Employees with Age > 30 (using SQL):\")\n",
        "spark.sql(\"SELECT Name, Age FROM employees WHERE Age > 30\").show()\n",
        "\n",
        "print(\"Average Salary (using SQL):\")\n",
        "spark.sql(\"SELECT AVG(Salary) as AverageSalary FROM employees\").show()\n",
        "\n",
        "print(\"Count of Employees by Gender (using SQL):\")\n",
        "spark.sql(\"SELECT Gender, COUNT(*) as Count FROM employees GROUP BY Gender\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF2ypU5_DZtO",
        "outputId": "19a916ba-26e5-4493-94fe-6342f72e5795"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Age > 30 (using SQL):\n",
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "|   Bob| 35|\n",
            "|Edward| 40|\n",
            "+------+---+\n",
            "\n",
            "Average Salary (using SQL):\n",
            "+-------------+\n",
            "|AverageSalary|\n",
            "+-------------+\n",
            "|      72000.0|\n",
            "+-------------+\n",
            "\n",
            "Count of Employees by Gender (using SQL):\n",
            "+------+-----+\n",
            "|Gender|Count|\n",
            "+------+-----+\n",
            "|Female|    2|\n",
            "|  Male|    3|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------\n",
        "# Part 3: MapReduce Example\n",
        "# -------------------------------------------\n",
        "\n",
        "# Example: Count the number of employees by gender using map-reduce in PySpark\n",
        "\n",
        "# Step 1: Map each record to a (key, value) pair where key = gender and value = 1\n",
        "gender_pairs = rdd.map(lambda x: (x[2], 1))\n",
        "\n"
      ],
      "metadata": {
        "id": "o_j4gSL5DfhQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Use reduceByKey to count occurrences of each gender\n",
        "gender_counts = gender_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"Employee count by gender (using MapReduce):\")\n",
        "print(gender_counts.collect())\n",
        "\n",
        "# -------------------------------------------\n",
        "# Tasks for Students:\n",
        "# -------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chG6dJRZDmsC",
        "outputId": "f242a041-c368-4f78-9a61-328fe1037b0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee count by gender (using MapReduce):\n",
            "[('Female', 2), ('Male', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Find the maximum salary in the dataset using RDD transformations.\n",
        "max_salary = rdd.map(lambda x: x[3]).reduce(lambda a, b: a if a > b else b)\n",
        "print(\"Maximum Salary:\", max_salary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j16r5jl0Dq6c",
        "outputId": "d5c97d40-56cb-4c0a-8c48-d647510cd0b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Salary: 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Find all employees whose salary is above the average salary using SQL.\n",
        "average_salary = spark.sql(\"SELECT AVG(Salary) as avg_salary FROM employees\").collect()[0][\"avg_salary\"]\n",
        "high_earners = rdd.filter(lambda x: x[3] > average_salary)\n",
        "print(\"Employees earning above the average salary:\")\n",
        "print(high_earners.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRNBMZwDDur2",
        "outputId": "dfe48a98-423b-4cb5-f7d8-9b2e9f251847"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees earning above the average salary:\n",
            "[('Bob', 35, 'Male', 80000), ('Edward', 40, 'Male', 100000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 3: Group employees by age range (e.g., 20-30, 31-40, etc.) and count them (students can use map-reduce or SQL).\n",
        "\n",
        "# Step 1: Map each employee to an age range\n",
        "age_range_rdd = rdd.map(lambda x: (f\"{(x[1] // 10) * 10}-{(x[1] // 10) * 10 + 9}\", 1))\n",
        "\n",
        "# Step 2: Reduce by age range\n",
        "age_range_counts = age_range_rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(\"Employee count by age range:\")\n",
        "print(age_range_counts.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZATAbwtDyCb",
        "outputId": "5dcbf533-75d1-4c9e-8628-b173461d5846"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee count by age range:\n",
            "[('20-29', 2), ('40-49', 1), ('30-39', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Clean up\n",
        "# -------------------------------------------\n",
        "# Stop the SparkSession to free resources\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "TW0pgkK1D2gT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PmRwkBGcD3eQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}